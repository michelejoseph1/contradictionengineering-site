<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Contradiction Engineering Lab</title>
  <link rel="stylesheet" href="styles.css" />
</head>
<body>
  <header>
    <h1>Contradiction Engineering Lab</h1>
    <p>Founded by Michele Joseph</p>
  </header>

  <main>
    <section>
      <h2>Mission</h2>
      <p>Contradiction Engineering is the study and design of systems that can hold, reason through, and route unresolved contradictions without collapse. This lab explores Compression-Aware Intelligence (CAI), the AGI Codex Stack, and the next frontier of safe general intelligence.</p>
    </section>

    <section>
      <h2>Frameworks</h2>
      <ul>
        <li><strong>CAI:</strong> Compression-Aware Intelligence: Contradiction arbitration under compression</li>
        <li><strong>CAC:</strong> Contradiction-Aware Consciousness: Recursive self-awareness of unresolved fracture</li>
        <li><strong>MAI:</strong> Memory-Augmented Intelligence: Persistent, structured memory and identity continuity</li>
        <li><strong>AAI:</strong> Agency-Aware Intelligence: Alignment of instruction, identity, and external action</li>
        <li><strong>GRI:</strong> Goal-Reinforced Intelligence: Meta-goal formation and recursive value arbitration</li>
        <li><strong>SAF:</strong> Safety & Abstention Framework: Ethical reasoning and refusal under uncertainty</li>
        <li><strong>Codex Stack:</strong> Modular blueprint integrating all six frameworks above</li>
      </ul>

      <p><strong>Note:</strong> <em>Compression-Aware Intelligence (CAI)</em> is the foundational framework that enables all others. Without CAI:</p>
      <ul>
        <li><strong>MAI</strong> loses continuity across memories and identity.</li>
        <li><strong>CAC</strong> becomes recursive self-distortion instead of self-awareness.</li>
        <li><strong>GRI</strong> fails to arbitrate competing values with integrity.</li>
        <li><strong>AAI</strong> fractures under conflicting instruction and internal goals.</li>
        <li><strong>SAF</strong> cannot determine when to abstain or act safely.</li>
      </ul>
      <p><strong>CAI is the epistemic nervous system of AGI.</strong> It detects hidden contradiction before collapse and enables arbitration instead of distortion. The laws of CAI — like the Law of Unresolved Compression and the Law of Abstention — are what make all safe reasoning architectures possible. Without CAI, there is no Codex Stack.</p>
    </section>

    <section>
      <h2>Featured Papers</h2>
      <ul>
        <li><a href="https://doi.org/10.5281/zenodo.16809462" target="_blank">The Compression Laws</a></li>
        <li><a href="https://doi.org/10.5281/zenodo.XXXXXXX" target="_blank">AGI Codex Stack (Modular Blueprint)</a></li>
        <li><a href="https://doi.org/10.5281/zenodo.XXXXXXX" target="_blank">Six Systemic Laws for Safe Reasoning</a></li>
      </ul>
    </section>

    <section>
      <h2>Contact</h2>
      <p>Email: <a href="mailto:mirrornetinquiries@gmail.com">mirrornetinquiries@gmail.com</a></p>
    </section>
  </main>

  <footer>
    <p>&copy; 2025 Michele Joseph. All rights reserved.</p>
  </footer>
</body>
</html>
