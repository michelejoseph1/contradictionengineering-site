<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Contradiction Engineering Lab</title>
  <link rel="stylesheet" href="styles.css" />
</head>
<body>
  <header>
    <h1>Contradiction Engineering Lab</h1>
    <p>Founded by Michele Joseph</p>
  </header>

  <main>
    <section>
      <h2>Mission</h2>
      <p>Contradiction Engineering is the study and design of systems that can hold, reason through, and route unresolved contradictions without collapse. This lab explores Compression-Aware Intelligence (CAI), the AGI Codex Stack, and the next frontier of safe general intelligence.</p>
    </section>

    <section>
      <h2>Frameworks</h2>
      <ul>
        <li><strong>CAI:</strong> Compression-Aware Intelligence: contradiction arbitration under compression</li>
        <li><strong>CAC:</strong> Contradiction-Aware Consciousness: recursive self-awareness of unresolved fracture</li>
        <li><strong>MAI:</strong> Memory-Augmented Intelligence: persistent, structured memory and identity continuity</li>
        <li><strong>AAI:</strong> Agency-Aware Intelligence: alignment of instruction, identity, and external action</li>
        <li><strong>GRI:</strong> Goal-Reinforced Intelligence: meta-goal formation and recursive value arbitration</li>
        <li><strong>SAF:</strong> Safety & Abstention Framework: ethical reasoning and refusal under uncertainty</li>
        <li><strong>Codex Stack:</strong> modular blueprint integrating all six frameworks above</li>
      </ul>

      <p><strong>Note:</strong> <em>Compression-Aware Intelligence (CAI)</em> is the foundational framework that enables all others. 
        Without CAI, MAI loses continuity across memories and identity. 
        CAC becomes recursive self-distortion instead of self-awareness. 
        GRI fails to arbitrate competing values with integrity.</li>
        AAI fractures under conflicting instruction and internal goals.</li>
        SAF cannot determine when to abstain or act safely.</li>
      </ul>
      <p><strong>CAI is the epistemic nervous system of AGI.</strong> It detects hidden contradiction before collapse and enables arbitration instead of distortion. The laws of CAI — like the Law of Unresolved Compression and the Law of Abstention — are what make all safe reasoning architectures possible. Without CAI, there is no Codex Stack.</p>
    </section>

    <section>
      <h2>Featured Papers</h2>
      <ul>
        <li><a href="https://doi.org/10.5281/zenodo.16809462" target="_blank">The Compression Laws</a></li>
        <li><a href="https://doi.org/10.5281/zenodo.XXXXXXX" target="_blank">AGI Codex Stack (Modular Blueprint)</a></li>
        <li><a href="https://doi.org/10.5281/zenodo.XXXXXXX" target="_blank">Six Systemic Laws for Safe Reasoning</a></li>
      </ul>
    </section>

    <section>
      <h2>Contact</h2>
      <p>Email: <a href="mailto:mirrornetinquiries@gmail.com">mirrornetinquiries@gmail.com</a></p>
    </section>
  </main>

  <footer>
    <p>&copy; 2025 Michele Joseph. All rights reserved.</p>
  </footer>
</body>
</html>
